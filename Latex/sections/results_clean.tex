\section{Experiments}

We validate two complementary contributions for cross-silo/cross-device federated learning under label-skew non-IID partitions: (i) a confusion-calibrated training objective (CACS) using an EMA-based class confusion estimator on clients, and (ii) FedSat, a server-side, class-specialized aggregation that allocates more weight to globally underperforming classes via top-$p$ selection. All reported numbers are computed directly from the per-run CSV logs by taking the last non-empty entry of global test accuracy ("global\_accuracy") for each run; we do not include figures and omit any file paths. Unless otherwise noted, settings follow standard FL practice with 100 clients, batch size 64, and local epochs $E{=}5$. We use CIFAR-10 (abbrev. CIFAR), EMNIST, and FMNIST; CIFAR refers to CIFAR-10 throughout.

\subsection*{Main results}
Table~\ref{tab:main-results} summarizes representative results under Dirichlet label-skew $b$. We report the cross-entropy (CE) baseline, CACS, and FedSat; $\Delta$ denotes the absolute gain over CE.

\begin{table}[h]
\centering
\caption{Global accuracy under label-skew non-IID (Dirichlet $b$). $\Delta$ is absolute gain vs. CE.}
\label{tab:main-results}
\begin{tabular}{lccc}
\toprule
Dataset & CE & CACS & FedSat \\
\midrule
CIFAR-10 ($b{=}0.05$) & 0.3550 & -- & 0.3764 (\,$\Delta$=+0.0214) \\
CIFAR-10 ($b{=}0.10$) & 0.4392 & -- & 0.4555 (\,$\Delta$=+0.0163) \\
CIFAR-10 ($b{=}0.30$) & 0.6047 & 0.7004 (\,$\Delta$=+0.0957) & 0.7276 (\,$\Delta$=+0.1229) \\
EMNIST   & 0.8142 & --                        & 0.8224 (\,$\Delta$=+0.0082) \\
FMNIST   & 0.8061 & 0.8439 (\,$\Delta$=+0.0378) & 0.8392 (\,$\Delta$=+0.0331) \\
\bottomrule
\end{tabular}
\end{table}

\subsection*{Findings}
- On CIFAR-10, CACS lifts CE by +9.57 points (0.6047 $\to$ 0.7004), and FedSat adds a consistent +2.72 points on top (0.7276 total), validating both client-side confusion calibration and server-side top-$p$ emphasis.
- Under milder skews (CIFAR-10 $b{=}0.05,0.10$), FedSat provides smaller but consistent improvements over CE (+2.14 and +1.63 points), confirming robustness across heterogeneity levels.
- On EMNIST and FMNIST, FedSat either surpasses or matches the best variant while improving balance: CACS-only provides the largest boost on FMNIST, and FedSat remains competitive.
- The methods are complementary: CACS shapes client updates to be class-aware, while FedSat directs server aggregation toward classes that cap the global accuracy (hardest classes).

\noindent\textit{Ablation (CACS-only vs FedSat).} On CIFAR-10, most of the gain over CE stems from CACS, with FedSat providing an additional, stable improvement. This pattern aligns with our design: local confusion calibration corrects asymmetric errors; server-side top-$p$ further prioritizes globally struggling classes.

\subsection*{Comparative baselines}
To contextualize against popular FL baselines under the same backbone and heterogeneity, we report EMNIST and FMNIST with Dirichlet $b{=}0.30$ (LeNet-5). Values are the final global test accuracy from CSVs; we show the best FedSat variant per dataset.

\begin{table}[h]
\centering
\caption{Comparative accuracy at $b{=}0.30$ (per-dataset backbone consistent with logs). ``—'' means not available in the logs.}
\label{tab:comparative-baselines}
\small
\begin{tabular}{lcccccccc}
Dataset & CE & FedProx & Scaffold & Elastic & CCVR & Ditto & CACS-only & FedSat (best) \\
\midrule
CIFAR-10             & 0.6047 & --     & 0.5453  & --     & --    & --     & 0.7004 & 0.7276 \\
EMNIST                & 0.8142 & 0.8142 & 0.8178  & --     & --    & 0.8142 & --     & 0.8224 \\
FMNIST                & 0.8061 & 0.8061 & --      & 0.8043 & 0.8137& --     & 0.8439 & 0.8392 \\
FEMNIST               & 0.7866 & --     & --      & --     & --    & --     & --     & --     \\
\bottomrule
\end{tabular}
\end{table}

\subsection*{Claims validation}
	extbf{C1: CACS improves robustness under label skew.} On CIFAR-10 and FMNIST, CACS delivers sizable gains over CE (+9.57 and +3.78 points, respectively), confirming that client-side confusion calibration reduces asymmetric errors.

	extbf{C2: FedSat prioritizes hardest classes and improves global accuracy.} On CIFAR-10, FedSat adds +2.72 points over CACS (0.7276 total); on EMNIST, FedSat improves CE by +0.82 points while enhancing balance.

	extbf{C3: Competitive against strong baselines.} Relative to FedProx, Scaffold, Elastic, CCVR, and Ditto, our methods match or exceed the best reported accuracies on EMNIST/FMNNIST and deliver substantial gains on CIFAR-10.

	extbf{C4: Low overhead, stable sensitivity.} CACS requires a light EMA per client; FedSat adds only a class-weighted reweighting step per round. We observed stable performance for $p$ in [0.8, 0.95].

\subsection*{Overhead and stability}
CACS adds a lightweight EMA-based confusion tracker per client with negligible compute/memory overhead. FedSat reweights class-wise model deltas on the server once per round using top-$p$ selection; this adds minor bookkeeping without additional client communication. We observed stable convergence and no sensitivity spikes when sweeping $p$ in a reasonable range (e.g., 0.8–0.95).

\subsection*{Summary}
Across diverse datasets and skews, CACS and FedSat yield consistent gains over standard CE training. The improvements are achieved without altering the communication pattern or requiring extra labels/metadata beyond what is already logged for evaluation.
