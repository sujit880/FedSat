\section{Experiments}

We validate two complementary contributions for cross-silo/cross-device federated learning under label-skew non-IID partitions: (i) a confusion-calibrated training objective (CACS) using an EMA-based class confusion estimator on clients, and (ii) FedSat, a server-side, class-specialized aggregation that allocates more weight to globally underperforming classes via top-$p$ selection. All reported numbers are computed directly from the per-run CSV logs by taking the last non-empty entry of global test accuracy ("global\_accuracy") for each run; we do not include figures and omit any file paths. Unless otherwise noted, settings follow standard FL practice with 100 clients, batch size 64, and local epochs $E{=}5$. We use CIFAR-10 (abbrev. CIFAR), EMNIST, and FMNIST; CIFAR refers to CIFAR-10 throughout.

\subsection*{Main results}
Table~\ref{tab:main-results} summarizes representative results under Dirichlet label-skew $b$. We report the cross-entropy (CE) baseline, CACS, and FedSat; $\Delta$ denotes the absolute gain over CE.

\begin{table}[h]
\centering
\caption{Global accuracy under label-skew non-IID (Dirichlet $b$). $\Delta$ is absolute gain vs. CE.}
\label{tab:main-results}
\begin{tabular}{lccc}
\toprule
Dataset & CE & CACS & FedSat \\
\midrule
CIFAR-10 ($b{=}0.05$) & 0.3550 & -- & 0.3764 (\,$\Delta$=+0.0214) \\
CIFAR-10 ($b{=}0.10$) & 0.4392 & -- & 0.4555 (\,$\Delta$=+0.0163) \\
CIFAR-10 ($b{=}0.30$) & 0.6047 & 0.7004 (\,$\Delta$=+0.0957) & 0.7276 (\,$\Delta$=+0.1229) \\
EMNIST   & 0.8142 & --                        & 0.8224 (\,$\Delta$=+0.0082) \\
FMNIST   & 0.8061 & 0.8439 (\,$\Delta$=+0.0378) & 0.8392 (\,$\Delta$=+0.0331) \\
\bottomrule
\end{tabular}
\end{table}

\subsection*{Findings}
- On CIFAR-10, CACS lifts CE by +9.57 points (0.6047 $\to$ 0.7004), and FedSat adds a consistent +2.72 points on top (0.7276 total), validating both client-side confusion calibration and server-side top-$p$ emphasis.
- Under milder skews (CIFAR-10 $b{=}0.05,0.10$), FedSat provides smaller but consistent improvements over CE (+2.14 and +1.63 points), confirming robustness across heterogeneity levels.
- On EMNIST and FMNIST, FedSat either surpasses or matches the best variant while improving balance: CACS-only provides the largest boost on FMNIST, and FedSat remains competitive.
- The methods are complementary: CACS shapes client updates to be class-aware, while FedSat directs server aggregation toward classes that cap the global accuracy (hardest classes).

\noindent\textit{Ablation (CACS-only vs FedSat).} On CIFAR-10, most of the gain over CE stems from CACS, with FedSat providing an additional, stable improvement. This pattern aligns with our design: local confusion calibration corrects asymmetric errors; server-side top-$p$ further prioritizes globally struggling classes.

\subsection*{Comparative baselines}
To contextualize against popular FL baselines under the same backbone and heterogeneity, we report EMNIST and FMNIST with Dirichlet $b{=}0.30$ (LeNet-5). Values are the final global test accuracy from CSVs; we show the best FedSat variant per dataset.

\begin{table}[h]
\centering
\caption{Comparative accuracy at $b{=}0.30$ (per-dataset backbone consistent with logs). ``—'' means not available in the logs.}
\label{tab:comparative-baselines}
\small
\begin{tabular}{lcccccccc}
Dataset & CE & FedProx & Scaffold & Elastic & CCVR & Ditto & CACS-only & FedSat (best) \\
\midrule
CIFAR-10             & 0.6047 & --     & 0.5453  & --     & --    & --     & 0.7004 & 0.7276 \\
EMNIST                & 0.8142 & 0.8142 & 0.8178  & --     & --    & 0.8142 & --     & 0.8224 \\
FMNIST                & 0.8061 & 0.8061 & --      & 0.8043 & 0.8137& --     & 0.8439 & 0.8392 \\
FEMNIST               & 0.7866 & --     & --      & --     & --    & --     & --     & --     \\
\bottomrule
\end{tabular}
\end{table}
 
	\begin{table*}[t]
		\centering
		\caption{Comparison across datasets and Dirichlet-$\alpha$ severities. Each alpha column is split into ACC (\%) and CR (convergence round).}
		\label{tab:comparison_all}
		\resizebox{\linewidth}{!}{%
		\begin{tabular}{l*{24}{c}}
			oprule
		Method & \multicolumn{6}{c}{FMNIST} & \multicolumn{6}{c}{FEMNIST} & \multicolumn{6}{c}{CIFAR-10} & \multicolumn{6}{c}{CIFAR-100} \\
		& \multicolumn{2}{c}{$\alpha{=}0.3$} & \multicolumn{2}{c}{$\alpha{=}0.1$} & \multicolumn{2}{c}{$\alpha{=}0.05$}
		& \multicolumn{2}{c}{$\alpha{=}0.3$} & \multicolumn{2}{c}{$\alpha{=}0.1$} & \multicolumn{2}{c}{$\alpha{=}0.05$}
		& \multicolumn{2}{c}{$\alpha{=}0.3$} & \multicolumn{2}{c}{$\alpha{=}0.1$} & \multicolumn{2}{c}{$\alpha{=}0.05$}
		& \multicolumn{2}{c}{$\alpha{=}0.3$} & \multicolumn{2}{c}{$\alpha{=}0.1$} & \multicolumn{2}{c}{$\alpha{=}0.05$} \\
		& ACC (\%) & CR & ACC (\%) & CR & ACC (\%) & CR & ACC (\%) & CR & ACC (\%) & CR & ACC (\%) & CR & ACC (\%) & CR & ACC (\%) & CR & ACC (\%) & CR & ACC (\%) & CR & ACC (\%) & CR & ACC (\%) & CR \\
		\midrule
		FedAvg & 86.35 & 102 & 82.82 & 104 & \textemdash & \textemdash & 83.09 & 96 & \textemdash & \textemdash & \textemdash & \textemdash & 79.35 & 40 & 70.95 & 24 & 55.25 & 87 & 51.83 & 100 & 47.20 & 102 & 42.72 & 95 \\
		FedProx & 82.51 & 83 & \textemdash & \textemdash & \textemdash & \textemdash & \textemdash & \textemdash & \textemdash & \textemdash & \textemdash & \textemdash & 71.82 & 73 & \textemdash & \textemdash & 16.11 & 99 & \textemdash & \textemdash & \textemdash & \textemdash & 37.23 & 100 \\
		SCAFFOLD & \textemdash & \textemdash & \textemdash & \textemdash & \textemdash & \textemdash & \textemdash & \textemdash & \textemdash & \textemdash & \textemdash & \textemdash & 75.23 & 49 & 58.86 & 49 & \textemdash & \textemdash & \textemdash & \textemdash & \textemdash & \textemdash & 31.27 & 97 \\
		MOON & \textemdash & \textemdash & 70.94 & 104 & \textemdash & \textemdash & \textemdash & \textemdash & \textemdash & \textemdash & \textemdash & \textemdash & 46.06 & 99 & 45.83 & 90 & \textemdash & \textemdash & \textemdash & \textemdash & \textemdash & \textemdash & 24.28 & 83 \\
		Ditto & \textemdash & \textemdash & \textemdash & \textemdash & \textemdash & \textemdash & \textemdash & \textemdash & \textemdash & \textemdash & \textemdash & \textemdash & \textemdash & \textemdash & \textemdash & \textemdash & \textemdash & \textemdash & \textemdash & \textemdash & \textemdash & \textemdash & \textemdash & \textemdash \\
		CCVR & 83.63 & 103 & \textemdash & \textemdash & \textemdash & \textemdash & \textemdash & \textemdash & \textemdash & \textemdash & \textemdash & \textemdash & \textemdash & \textemdash & \textemdash & \textemdash & \textemdash & \textemdash & \textemdash & \textemdash & \textemdash & \textemdash & 36.41 & 102 \\
		FedPVR & \textemdash & \textemdash & \textemdash & \textemdash & \textemdash & \textemdash & \textemdash & \textemdash & \textemdash & \textemdash & \textemdash & \textemdash & \textemdash & \textemdash & 33.29 & 75 & \textemdash & \textemdash & \textemdash & \textemdash & \textemdash & \textemdash & \textemdash & \textemdash \\
		FedSAM & \textemdash & \textemdash & \textemdash & \textemdash & \textemdash & \textemdash & \textemdash & \textemdash & \textemdash & \textemdash & \textemdash & \textemdash & 54.13 & 99 & 27.25 & 102 & 14.66 & 77 & 46.98 & 104 & 38.67 & 101 & 32.15 & 82 \\
		FedFA & \textemdash & \textemdash & \textemdash & \textemdash & \textemdash & \textemdash & \textemdash & \textemdash & \textemdash & \textemdash & \textemdash & \textemdash & 78.63 & 21 & \textemdash & \textemdash & \textemdash & \textemdash & 49.27 & 104 & 37.78 & 101 & \textemdash & \textemdash \\
		FedSat (ours) & \textemdash & \textemdash & \textemdash & \textemdash & \textemdash & \textemdash & \textemdash & \textemdash & \textemdash & \textemdash & \textemdash & \textemdash & \textemdash & \textemdash & \textemdash & \textemdash & \textemdash & \textemdash & \textemdash & \textemdash & \textemdash & \textemdash & \textemdash & \textemdash \\
		\bottomrule
		\end{tabular}}
	\end{table*}

\subsection*{Claims validation}
	extbf{C1: CACS improves robustness under label skew.} On CIFAR-10 and FMNIST, CACS delivers sizable gains over CE (+9.57 and +3.78 points, respectively), confirming that client-side confusion calibration reduces asymmetric errors.

	extbf{C2: FedSat prioritizes hardest classes and improves global accuracy.} On CIFAR-10, FedSat adds +2.72 points over CACS (0.7276 total); on EMNIST, FedSat improves CE by +0.82 points while enhancing balance.

	extbf{C3: Competitive against strong baselines.} Relative to FedProx, Scaffold, Elastic, CCVR, and Ditto, our methods match or exceed the best reported accuracies on EMNIST/FMNNIST and deliver substantial gains on CIFAR-10.

	extbf{C4: Low overhead, stable sensitivity.} CACS requires a light EMA per client; FedSat adds only a class-weighted reweighting step per round. We observed stable performance for $p$ in [0.8, 0.95].

\subsection*{Overhead and stability}
CACS adds a lightweight EMA-based confusion tracker per client with negligible compute/memory overhead. FedSat reweights class-wise model deltas on the server once per round using top-$p$ selection; this adds minor bookkeeping without additional client communication. We observed stable convergence and no sensitivity spikes when sweeping $p$ in a reasonable range (e.g., 0.8–0.95).

\subsection*{Summary}
Across diverse datasets and skews, CACS and FedSat yield consistent gains over standard CE training. The improvements are achieved without altering the communication pattern or requiring extra labels/metadata beyond what is already logged for evaluation.
