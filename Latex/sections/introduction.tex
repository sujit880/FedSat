% % ---------------------------- INTRODUCTION ----------------------------
% \section{Introduction}
% \IEEEPARstart{F}{ederated} Learning (FL) trains a global model by aggregating updates from clients holding private data. Real-world FL deployments are far from IID: clients experience severe \emph{heterogeneity} in label distribution (label skewness), may \emph{miss entire classes}, and often have widely varying dataset sizes (quantity skewness). These conditions induce \emph{client drift}, slower and unstable convergence, and disproportionate errors on minority or intrinsically hard classes.

% Stability-oriented methods such as FedAvg~\cite{fedavg}, FedProx~\cite{fedprox}, FedDyn~\cite{fedDyn}, MOON~\cite{MOON}, and SCAFFOLD~\cite{scaffold} mitigate drift through averaging, proximal or dynamic regularizers, contrastive alignment, or control variates. However, they are largely \emph{class-agnostic}: all errors are penalized equally, ignoring the \emph{pairwise} misclassification structure that naturally emerges in non-IID settings (e.g., dogs vs. wolves, trucks vs. automobiles). Cost-sensitive objectives like Focal Loss~\cite{lin2017focal} and Class-Balanced Loss~\cite{cui2019class} help with imbalance but are \emph{static} and class-centric; they do not leverage the evolving \emph{confusion graph} between specific class pairs and thus cannot target persistent, asymmetric confusions.

% We introduce \textbf{FedSat}, an FL framework that couples:
% \begin{itemize}
% 	\item \textbf{Confusion-Calibrated Cross-Entropy} (\lossabbr): a local objective that augments cross-entropy using a \emph{Cost-of-Misclassification} term derived from an \emph{EMA soft confusion} estimator, and further calibrates logits with a \emph{confusion-modulated prior} and a \emph{confusion-derived margin}. These components are lightweight, differentiable, and reduce to standard CE when confusion is absent.
% 	\item \textbf{Class-specialized, top-$p$ aggregation}: a server-side procedure that prioritizes \emph{struggler classes} (those with high misclassification rates) using only compact per-class signals from clients. For the top-$p$ struggling classes, the server computes \emph{class-specific client weights} and aggregates specialized models, then averages them to update the global model.
% \end{itemize}

% This coupling yields faster and more stable convergence, stronger performance on minority and hard classes, and improved fairness across clients, while incurring negligible communication overhead. Importantly, FedSat \emph{does not} rely on control variates or share raw gradients/confusion matrices; only model parameters and a compact per-class struggler vector are communicated, preserving compatibility with DP and secure aggregation.

% \noindent\textbf{Why confusion?} In non-IID regimes, the global objective is skewed by client-local exposures to labels. The error surface develops \emph{structured valleys} aligned with confusable pairs, so treating errors uniformly under-utilizes informative structure. FedSat estimates a \emph{soft confusion graph} online via an EMA of predicted probabilities against labels, then shapes the loss: it raises the partition function for confusable negatives (\emph{CoM-CE}), adds a small Bayes-like prior bias guided by observed label frequencies, and subtracts per-class margins on confusable non-targets. Together, these operations encourage within-class separation and between-class disambiguation precisely where the model struggles most.

% \noindent\textbf{Contributions.} Our main contributions are:
% \begin{itemize}
% 	\item A principled and simple \lossabbr{} objective that instantiates a confusion-aware \emph{Cost-of-Misclassification Cross-Entropy} (CoM-CE), a confusion-modulated prior, and a confusion-derived margin—all built from on-device EMA statistics with warm-up gating.
% 	\item A \emph{struggler-aware}, class-specialized top-$p$ aggregation mechanism that uses inverse-struggle weights to focus updates on the hardest classes without additional gradient traffic.
% 	\item Properties and intuitions suggesting faster progress on non-IID data and improved minority-class robustness, with ablations that isolate the roles of loss shaping and aggregation.
% 	\item Extensive experiments on MNIST, CIFAR-10/100 under label skew, missing classes, and quantity skew, demonstrating consistent gains over strong baselines including FedAvg, FedProx, FedDyn, MOON, SCAFFOLD, FedLC, Elastic, FedAvgM, and FedPVR.
% \end{itemize}

% We next formalize the confusion-calibrated local objective, the struggler signal and class-specialized aggregation, and provide empirical and analytical evidence for FedSat's effectiveness under heterogeneity.


%%% v2
\section{Introduction}
Federated Learning (FL)~\cite{mcmahan2017communication} has emerged as a promising paradigm for distributed model training, enabling multiple clients to collaboratively learn a shared global model without exchanging raw data. This decentralized paradigm provides a practical balance between privacy and utility, making it suitable for large-scale applications such as healthcare, IoT, and multi-organization collaboration. Despite these advantages, the performance of FL is often constrained by \textit{data heterogeneity}—a fundamental challenge arising from the non-IID nature of client datasets.
In practice, data heterogeneity manifests in several forms \cite{}: (i) \textit{label skewness}, where class distributions vary widely among clients; (ii) \textit{missing classes}, where certain classes are completely absent on some clients; and (iii) \textit{quantity skewness}, where the amount of data per client differs significantly. These conditions lead to biased local updates, slow convergence, and poor global generalization. Moreover, they exacerbate disparities between clients, resulting in overfitting to dominant data sources and underperformance on rare or minority classes. Effectively addressing these intertwined issues remains one of the central goals in federated optimization.

Over the past few years, numerous efforts have been made to mitigate the adverse effects of data heterogeneity. The seminal work FedAvg~\cite{mcmahan2017communication} introduced the standard aggregation framework for FL, but its simple averaging scheme struggles under strong non-IID conditions. Subsequent methods such as FedProx~\cite{li2020federated}, FedDyn~\cite{acar2021feddyn}, and FedDC~\cite{gao2022feddc} incorporated regularization terms to stabilize local optimization and align client objectives with the global model. Similarly, control-variance and momentum-based approaches such as SCAFFOLD~\cite{karimireddy2020scaffold}, FedPVR~\cite{li2023partial}, and FedAvgM~\cite{hsu2019measuring} aim to reduce the divergence between local and global gradients. Representation-driven methods such as MOON~\cite{li2021model} and FedBN~\cite{li2021fedbn} further enhance consistency by aligning client representations or applying normalization-based calibration. While these frameworks alleviate client drift and improve stability, they remain inherently \textit{class-agnostic}: the aggregation process does not distinguish which classes or confusion pairs contribute most to overall model degradation.

In parallel, several cost-sensitive and class-balancing strategies have been explored to counteract label skewness. Focal Loss~\cite{lin2017focal} emphasizes hard samples by down-weighting easy ones, and Class-Balanced Loss~\cite{cui2019class} compensates for class frequency imbalance using effective-number weighting. In the federated context, FedLC~\cite{zhang2022fedlc} adjusts logits based on label distributions to handle skewed class priors. Despite their success in mitigating imbalance, these losses are inherently static and class-centric—they assume symmetric error structures and ignore the evolving nature of pairwise confusions (e.g., confusing class $A \rightarrow B$ being more frequent than $B \rightarrow A$). Moreover, while they improve local calibration, they are not coupled with the aggregation mechanism that dictates how local updates influence the global model. As a result, such methods fail to jointly address the asymmetric, evolving, and class-specific challenges inherent in federated environments.

A closer look at these limitations reveals an important insight: misclassification patterns themselves encode valuable information about the underlying heterogeneity. Persistent confusions between particular classes reflect regions where the federation collectively struggles, often due to biased or insufficient class representation. However, most existing FL frameworks neglect these evolving confusion dynamics—neither adapting local objectives based on confusion statistics nor steering aggregation according to class-specific difficulty. This disconnect motivates the need for an approach that can (i) dynamically model pairwise misclassification asymmetries at the client level, (ii) summarize class-wise learning difficulty through compact signals, and (iii) exploit these signals to guide class-prioritized aggregation without increasing communication overhead.

To this end, we propose \textbf{FedSat}, a federated learning framework designed to statistically couple local confusion modeling with global aggregation prioritization. FedSat introduces two complementary components that work in synergy. First, the \textit{Confusion-Calibrated Cross-Entropy (C3E)} serves as a dynamic, cost-sensitive local objective. It maintains an exponentially moving average (EMA) of a soft confusion matrix that encodes pairwise asymmetric misclassification costs $\Delta_{y,j} \ge 0$, allowing each client to adaptively penalize persistent or systematic confusions. In addition, C3E incorporates a prior-bias term to correct for class frequency shifts and a confusion-derived margin to sharpen decision boundaries between often-confused classes. Second, on the server side, FedSat employs a \textit{class-specialized top-$p$ aggregation} scheme. Each client produces a compact \textit{struggler signal}—a per-class summary of residual difficulty derived from its confusion statistics. The server then identifies the top-$p$ most struggling classes across the federation and aggregates client models with higher weights assigned to those performing better on these critical classes. The final global model is obtained by averaging these class-specialized submodels, thus directing learning towards the most error-prone regions.

This tight coupling between confusion-aware local learning and class-prioritized aggregation enables FedSat to continuously align local optimization with global objectives, without any additional communication cost. By exploiting the evolving structure of confusion, FedSat enhances minority-class accuracy, accelerates convergence, and achieves greater fairness across clients and classes in diverse non-IID settings.

The main contributions of this work are summarized as follows:
\begin{itemize}
    \item We propose \textbf{FedSat}, a novel federated learning framework that unifies confusion-aware local objectives with class-specialized aggregation to address label skewness, missing classes, and quantity skewness in a communication-efficient manner.
    \item We introduce the \textbf{Confusion-Calibrated Cross-Entropy (C3E)}, a dynamic loss function that learns pairwise asymmetric misclassification costs from an EMA of the soft confusion matrix, augmented with prior bias and confusion-margin calibration.
    \item We develop a \textbf{struggler-driven top-$p$ aggregation} mechanism that prioritizes updates from clients performing better on the most challenging classes, thereby enhancing global robustness and fairness.
    \item We provide theoretical analysis establishing local improvement guarantees for C3E and faster expected convergence for the FedSat global model.
    \item Through extensive experiments on MNIST, CIFAR-10, and CIFAR-100 using multiple architectures (MLP, LeNet-5, ResNet-8, ResNet-18), we demonstrate that FedSat consistently outperforms state-of-the-art baselines—including FedAvg, FedProx, FedDyn, SCAFFOLD, MOON, FedLC, FedPVR, and Elastic—in accuracy, convergence speed, and minority-class performance under diverse non-IID regimes.
\end{itemize}

% In summary, FedSat bridges the long-standing gap between local confusion modeling and global aggregation by transforming misclassification statistics into actionable signals that drive both local learning and server aggregation. This statistical coupling enables robust, fair, and efficient learning under the complex data heterogeneity that defines real-world federated systems.
